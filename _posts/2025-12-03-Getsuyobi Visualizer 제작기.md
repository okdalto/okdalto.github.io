---
title: "'G e t s u yo b i' Visualizer 제작기"
date: 2025-12-03T15:05:14+09:00
categories:
  - 작업
tags:
  - AI
  - Visualizer
---

이번에 좋은 기회로 'suiimeetsyou'님이 작곡한 'G e t s u yo b i' 라는 곡의 Visualizer를 제작하게 되었다. 마침 창작에 목마른 시점인지라 창작욕을 마구 발산해보자고 다짐했고, 아주 재미있게 작업했다.

나는 이 작업에서 요새 잘나간다는 바로 그 'AI'를 활용했다. 그렇다. 나는 무려 AI 아티스트이다. 요즘엔 AI 아티스트로 자신을 정의하는 사람들이 참 많다. 그럼에도, AI라는 매체 자체를 탐구하는 작업은 드물다. 일반적으로 AI는 그저 창작의 도구이지 그것 자체가 목적은 아닌 것으로 여겨진다. 이 점이 내게는 늘 미스테리다. 단순히 AI를 도구로 사용했다고 AI 아티스트라 한다면, 왜 포토샵 아티스트나 붓 아티스트라는 표현은 쓰지 않을까? 마치 첼로를 사용하면 첼리스트라고 부르는 것처럼, AI를 사용하니까 AI 아티스트인 걸까?

그래서 나는 지금의 AI 아티스트라는 말이 과도기적 표현이라고 생각한다. 디지털 작업이 생경하던 과거에는 디지털 작업이 특별했기 때문에 디지털 아티스트라는 표현을 사용하는 것에 의미가 있었지만, 지금은 디지털 도구를 사용하지 않는 창작자를 찾는 쪽이 더 어려울 정도이기에 단순히 컴퓨터를 사용하는 아티스트라고 하여 이 표현을 쓰는 경우는 거의 없다. 마찬가지로, AI가 창작 도구로서 일반화되면 AI를 작업에 쓰는 것은 당연해지고, AI 아티스트라는 표현도 자연스레 사라질 것이다.

내 작업에서 내게 중요한 것은 딥 러닝이라는 기술이 가진 구조, 잠재 공간의 성질, 숨겨진 조형 요소들을 직접 탐험하고, 그 내부의 작동 원리를 창작의 재료로 삼는 일이다. 이번 Visualizer는 그런 의미에서 단순히 AI를 사용해서 끝인 결과물이 아니라, 뉴럴 네트워크라는 매체의 물성과 조형 언어를 탐구한 작업에 가깝다. 그러니까 이건... AI 아트인 셈이다.

잘난척으로 시작한 이 작업기에서는 AI라는 매체를 어떻게 탐구하였는가를 설명하기 위해 아주 기초적인 개념부터 차근차근 다뤄보려 한다.

### Generative AI 

컴퓨터는 무엇인가? 'Compute'는 '계산하다' 라는 뜻을 가지고 있다. 그렇다. 컴퓨터는 기본적으로 계산기이다. 그렇기에 같은 입력을 넣으면 언제나 같은 출력이 나온다. 계산기에 1+1을 넣었는데 3이 나온 경험이 있는가? 입력이 정해지면 네트워크의 출력은 이미 결정된 것이나 마찬가지다.

이것은 우리가 일반적으로 기대하는 생성의 개념과는 거리가 멀다. 생성이란 기존에 없던 것을 만들어내는 일이고, 생성의 주체는 약간 바뀐 입력에서도 다양한 결과를 낼 수 있어야 한다. 하지만 컴퓨터는 기본적으로 결정론적 계산기이다. 그럼에도 지금의 생성 모델은 무언가를 무에서 생성해내는 능력이 있는 것처럼 보인다. 어떻게 된 일인가?

### Noise

컴퓨터가 새로운 것을 만드는 것처럼 보이기 위해서는 불확실함이라는 재료를 사용한다. noise의 등장이다. 이 noise라고 부르는 것은 특정한 확률 분포를 따르는 무작위 값이다. noise에는 종류가 많다. 우리가 흔히 아는, 모든 값이 같은 확률을 갖는 Uniform noise도 있고, Salt-and-Pepper noise처럼 극단값만 가끔 튀는 노이즈도 있지만, 생성 모델에서는 그중에서도 Gaussian distribution에서 샘플링한 Gaussian noise를 유독 많이 사용한다. 왜 하필 Gaussian noise일까?

Gaussian distribution은 자연계의 수많은 현상이 따르는 분포이다. 사람의 키, 시험 점수, 센서의 오차, 바람의 세기처럼 다양한 요인이 조금씩 더해져 만들어지는 현상은 대체로 Gaussian distribution 형태를 띤다. 복잡한 원인이 겹쳐질수록 결과의 분포는 Gaussian distribution에 가까워진다. 왜 그렇게 되느냐? 세상이 그렇게 생겨먹었다. 이걸 증명한 것이 Gauss의 Central Limit Theorem이다.

또한, Gaussian distribution은 계산적으로 매우 다루기 좋다. 매끄럽고, 선형 변환 뒤에도 모양이 크게 망가지지 않기 때문에 딥러닝이 여러 층을 통과하는 과정에서 다루기 쉽다. 즉, Gaussian distribution은 자연스럽고, 다루기 쉽고, 안정적이다. 따라서 Gaussian noise를 사용하는 것은 생성 모델에서 일종의 표준처럼 자리잡고 있다.

### Method

Gaussian noise를 사용한다는 것은 이제 알겠는데, 생성에 사용될 네트워크의 구조나, 학습 방식은 어떻게 가져가는 것이 좋을까? 매우 단순하게는 MLP를 써서 noise를 입력으로 받아 벡터의 크기를 점점 키운 뒤 결과물을 reshape하여 이미지를 만드는 방법이 있을 수 있다. 이 방법은 단순한 만큼 결과는 좋지 않을 것이다.

좀 더 발전한 것으로는 convolution을 사용하는 방법이 있다. convolution은 주변 픽셀의 값을 곱하고 더해 이미지의 특징을 추출하는 연산이다. 이미지를 생성하는 과정에서는 이와 반대되는 방식으로, 저해상도의 특징 정보를 점점 더 고해상도의 이미지로 확장하는 transposed convolution을 사용한다. 이 방식은 어떤 이미지를 생성할 때 주변 픽셀의 값까지 고려하기 때문에 단순히 MLP를 연결한 것보다는 이미지 생성에서 좀 더 잘 작동하는 모습을 보인다.

여기에 GAN(Generative Adversarial Networks)의 학습 방식을 차용한 것을 DCGAN(Deep Convolutional Generative Adversarial Networks)이라고 한다. GAN의 아이디어는 심플하지만 강력하다. Generator(생성자)와 생성을 판단하는 discriminator(판별자)를 두어 학습하는 것이다. 비유하자면, generator는 위조지폐범이고, discriminator는 진짜 돈과 가짜 돈을 구별하려는 감식관이다. generator는 discriminator를 속이기 위해 더 정교한 이미지를 만들어내고, discriminator는 generator의 속임수를 잡아내기 위해 더 엄격해진다. 이 두 네트워크가 서로 경쟁하며 성장하는 과정에서 모델은 점점 더 그럴 듯한, 그러니까 원래 데이터 분포를 잘 근사한 이미지를 만들 수 있게 된다. 이것이 바로 GAN의 핵심 아이디어다.

이 DCGAN은 convolution 기반 GAN의 정석처럼 여겨졌지만, 여전히 이미지의 퀄리티는 썩 만족스러운 수준은 못 되었다. 그리고, StyleGAN이 등장했다.

### StyleGAN

기존의 DCGAN에서는 Gaussian noise를 네트워크 맨 앞에 넣기만 하면, 생성 과정 전체에서 그 정보가 밀려 내려왔다. 그러나 StyleGAN에서는 다수의 MLP를 써서 z를 latent space로 변환한다. 이것을 disentangle이라고 표현하는데, Gaussian distribution으로 표현된 noise의 공간을 Generator가 더 잘 해석할 수 있는 공간으로 풀어헤치는(disentangle) 것이라 볼 수 있기 때문이다. 이렇게 획득한 벡터를 w라 하는데 이 w를 네트워크의 각 해상도 단계마다 컨트롤 신호처럼 주입한다. 이렇게 normalization의 파라미터로 w를 사용하여 신호를 주입하는 방식을 Adaptive Instance Normalization, AdaIN이라 부른다. 이 구조를 통해 이미지의 전체 구조, 중간 디테일, 세부 텍스처의 각 스타일을 서로 독립적으로 제어할 수 있게 됐고, 이미지 생성 퀄리티 또한 비약적으로 상승했다.

### StyleGAN2

이후, StyleGAN2가 등장했다. StyleGAN2에서는 AdaIN을 없애고, w를 통해 weight를 직접적으로 modulation하는 방식을 채택하여 기존 StyleGAN의 과도한 normalization을 억제해 이미지 퀄리티를 증가시켰다. 이후에 antialiasing이 적용된 StyleGAN3가 나왔지만, 이미지 생성 퀄리티만큼은 StyleGAN2를 이기기 쉽지 않았다. 이 작업에서는 바로 이 StyleGAN2를 사용했다. 이유는 간단하다. 바닥부터 다 뜯어본 경험이 있기 때문에 익숙하기 때문.

### Training

과일이 등장했으면 좋겠다는 suiimeetsyou 님의 요청에 따라, 과일 이미지를 수집할 방법을 고안했다. 일단, GPT를 이용해서 다양한 과일의 목록을 작성했다. 이 목록을 통해 흰 배경에 놓인 과일 prompt를 만들었고, 이 프롬프트를 오픈 소스 이미지 생성 모델에 넣어 학습에 사용할 과일 이미지를 엄청나게 많이 만들었다.

이 데이터로 StyleGAN을 학습했다. 학습은 간단하다. runpod 서버를 하나 빌리고 torch로 작성한 코드를 구동시켜 학습을 진행했고, 기학습된 체크포인트를 사용해 transfer learning하여 학습 시간을 단축시켰다.

### Latent Travel

Latent vector를 네트워크에 넣으면 이미지가 나온다. 이 구조에서 네트워크는 latent space의 한 점을 이미지 공간의 한 점에 대응시키는 mapping function의 역할을 한다. latent space에서 매끄럽게 점을 이동시키면, 매핑을 통해 대응되는 이미지 또한 이동하는데, 그 움직임이 매우 기묘하다. 이것을 latent travel이라고 하는 듯 하다.

시각화 초반부에 내가 사용한 시각적 트릭은 이를 바탕으로 한다. 음악에 맞춰 latent vector w를 이동시키는 것이다. z를 이동시켜도 비슷한 시각효과를 줄 수 있지만, disentangle된 w를 사용하는 쪽이 표현력이 더 풍부해 이쪽을 사용했다. 

아무튼 요점은 latent space의 한 점을(그것이 z이든, w든) 이동시켰다는 것이다. 여기서 문제가 생긴다. w를 이동시키는 것은 좋은데, 그것이 어떤 경로로 이동하게 할 것인가?

### Curve

아주 간단하게는 이렇게 시도해볼 수 있다. Latent space 안에서 여러 개의 위치(w)를 생성하고, 위치를 직선 경로로 잇고, 그 경로 위에서 linear하게 w가 이동하게 만드는 것이다. 매우 간단해서 만들기 쉬운 방법이지만, 이 방식에는 문제가 있다. 직선이기 때문에 두 경로가 맞닿는 곳에서(경로가 꺾이는 부분에서) 전후 이미지가 크게 달라진다는 점이다. 따라서, 부드러운 변화를 위해서는 곡선을 사용해야 한다.

곡선의 종류는 매우 다양하다. 아마 컴퓨터 그래픽스를 잘 배운 사람이라면, 수많은 종류의 곡선 이름이 떠오를 것이다. Hermite Curve, Bezier Curve, Catmull-Rom Spline... 등등. 나는 이 중에서 Catmull-Rom Spline을 선택했다. 이유는 간단하다. tangent값을 주지 않고, 위치만 주더라도 부드러운 곡선이 만들어지기 때문이다. 이에 따라 나는 n개의 w값을 주고, looping하는 Catmull-Rom Spline을 만들고, 그 curve 위에서 0~1의 t 값을 통해 끝과 끝이 이어진 무한한 curve 위의 w를 샘플링하는 클래스를 만들었다. 이렇게 샘플링한 값을 사용하면, 이미지가 부드럽게 변화하고, 커브의 시작과 끝이 연결되어 있기 따문에 영상의 앞뒤가 무한히 연결된다. 음악에서 뽑아낸 amplitude를 이용해서 이 curve 위의 움직임(즉, t의 변화)을 제어하게 만들면, 음악에 따라 변화하는 과일의 영상을 쉽게 뽑아낼 수 있다.

### Feature

Image feature는 무엇일까? StyleGAN이 이미지를 생성하는 데 필요한 값은 w 외에도 convolution 시작 단계에 사용되는 noise image가 있다. StyleGAN은 이 작은 noise에 다수의 upsample과 modulated convolution을 적용하며 이미지 사이즈를 점점 키워내는데, 이 과정에서 생성되는 중간 결과물을 feature라고 한다. 이 feature를 이리저리 뒤틀면, 그 편집 결과는 최종 생성 결과물에도 영향을 끼친다. 내가 이 작업 후반부에서 시도한 것은 upsamle 단계를 잘라내 중간 feature를 꺼내고, 이 feature를 여러 방식으로 왜곡시킨 뒤에 나머지 생성 과정을 거치게 한 것이다. Feature에 slit-scan을 적용한다거나, 회전시키거나, 여러 조각으로 쪼개서 각각 위아래로 움직인다거나 하는 것이다. 여기에서도 음악에서 뽑아낸 amplitude를 이용해서 움직임을 제어할 수 있다. 움직임이라는 것은 결국 숫자로 표현할 수 있는데, 따라서 숫자인 amplitude 값 또한 이 움직임에 이리저리 곱하고 더해질 수 있기 때문이다. 

이렇게 나온 결과물로 내용을 채운 것이 바로 이 Visualizer이다. 원리를 알고 나니 아주 간단하지 않은가? GAN의 구조나 latent space의 성질, 그리고 feature-level manipulation 같은 기술적인 요소들은 겉보기엔 어려워 보이지만, 원리를 이해한 순간부터는 오히려 창작의 재료가 된다. AI는 그저 편리한 자동화 도구가 아니라, 그 구조 자체가 또 하나의 재료이며, 창작자는 그 재료를 어떻게 다루느냐에 따라 전혀 다른 표현을 만들 수 있다. 